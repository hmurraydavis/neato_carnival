#Neato Carnival Project
Kaitlin Gallagher, Anne LoVerso, Halie Murray-Davis


##Video Demonstrations:
Working Dominoes:
http://youtu.be/Qy7JWnR5oHk

Working tunnel:
http://youtu.be/xrzc4VkT8hg 

Tunnel to dominoes
http://youtu.be/uIWvGxosoLM 

##Project Goal
Create an obstacle course for the Neato, so it can be placed in a world and immediately be able to find obstacles, identify them, complete the “task”, and move on to the next obstacle.  It should be able to do this autonomously without interaction from humans guiding it to or through an obstacle.

The three obstacles that we implemented are a bridge, a tunnel, and a domino station.  The bridge obstacle challenges the Neato robot to cross the bridge without falling off the edges.  The Neato has to pass through the tunnel without bumping the walls.  Because tunnels are dark, it must use only Lidar laser scan data to guide it.  Finally, the Neato’s job on the domino obstacle is to only knock over red dominoes without harming the blue dominoes.

##High-Level Solution
We implemented our solution using a finite-state machine, with two distinct sections depending on whether the Neato is “in” an obstacle or not.  The state machine has the Neato in “finding” mode where it simply spins in a circle until it finds an obstacle in its sight.  Then it travels into position in front of the obstacle and switches into a state machine for that specific obstacle.  When the obstacle completes, it switches back into finding mode until the robot sees another obstacle.

We solved the problem of identifying obstacles by using fiducials and the ar_pose ROS package.  The ar_pose package is able to recognize fiducials within the Neato’s camera vision and compute the tf transform between the robot and that location.  It can also identify which fiducial it is seeing, so the Neato knows whether it is looking at a fiducial corresponding to, say, the bridge obstacle compared to the domino obstacle.  From there, we needed to solve the problem of navigating the Neato into position in front of the obstacle, ready to start.  For this, we used the move_base package in ROS, which receives a pose or tf transform, and navigates the robot to the goal pose.  This allows us to align the robot correctly, relative the fiducial, for each obstacle.

##Design Decision
Before our design review, we had been planning to have each obstacle’s finite state machine responsible for navigating the robot to the correct starting position, using some combination of Lidar and computer vision to allow the robot to figure out where it was and where it needed to go.  We presented a somewhat iffy method for lining up the robot with the entrance to the tunnel, and we were definitely uncertain in our ability to get the robot ready to even start completing the obstacles.  After we received some feedback on that, we poked around for better alternatives and found the move_base package, which was immensely helpful in overcoming that roadblock and actually focusing on the logic to allow the robot to complete the obstacles.

The choice to use fiducials, although it was made at the very beginning of the project, was a major design decision as well.  The fiducials again allowed us to focus on coding the logic behind completing the obstacles, because we didn’t have to worry about optical object recognition or other methods of identifying obstacles. In light of our difficulty finding the edges of the bridge due to changing room conditions, this was a very good design decision! 

##Code Structure
Our code is structured into a multi-level finite state machine.  The robot’s default state is the RideFinder class FSM, which goes through each step of locating a fiducial within its environment, sending the coordinate transform to the move_base package, traveling to the fiducial, and arriving at the correct distance and orientation for that obstacle.  Once it determines that it is in the correct predetermined location relative to the fiducial for that obstacle, it switches into the subsystem FSM for that obstacle.  That FSM operates until it considers its state to be “done”, at which point the whole FSM returns to finding mode and the loop restarts.

The finite state machines for each obstacle depend on that obstacle, but they are all built on the same general structure.  The overall main function instantiates an instance of each obstacle at the beginning, which begin in their “wait” states until they are needed.  Then, when it is time to switch into an obstacle’s FSM, it calls the “do” method for that obstacle.  The obstacle object then switches its state to whatever its first state is, and operates.  Each object also gets reset after its “done” state so it is ready in case the robot finds it again.

###Tunnel FSM
The tunnel finite state machine is fairly simple, because it is already lined up ready to enter the tunnel.  Therefore, the only major state is the “drive thru” state.  This state begins by using Lidar to build a 360-degree view of the environment.  We chose to allow the robot to only use Lidar for the tunnel to simulate as if it was truly in a dark tunnel and computer vision would be useless.  The Lidar builds a 360-element binary array, with each element representing whether or not there is an obstacle in that direction.  It then looks to the front and searchs for a window of open space with no walls in the way.  It finds the largest such window, and locates the middle of it.  Then it simply uses a proportional controller to direct itself towards the middle of that window.  When it sees the window to be greater than 200 degrees wide, it considers itself to be out of the tunnel, and switches to its done state.

###Domino FSM
The Domino begins in position looking at a line of colored dominoes, and its task is to only knock over red dominoes, while leaving the blue dominoes untouched.  It begins by taking a frame from its camera view, and using OpenCV to run color thresholding for red and blue.  It then uses contours to find the center of blobs of color.  It compares these to the locations of the fiducials it sees, and uses this to determine which fiducials it should remember (those with locations next to red centers) and which it can ignore (those with locations next to blue centers).  This completes the first state of locating dominoes.  Then, it saves each fiducial’s position as a pose, and uses move_base to run through its list of red locations, traveling to each one to knock over the domino.  It finishes by returning to the origin through another saved pose.

###Bridge Behavior
Traversing the bridge proved troublesome. Changing light conditions made finding the red strips which denoted the edge of the bridge extremely difficult. The method worked for selected moments in specific rooms, but more general applicability would require much more time refining parameters and, possibly, approach before a robust algorithm and process was obtained. Significant iterations on this process have already occurred. 

First, basic smoothing was performed on the image. This was done to reduce the effect of noisy features and only make significant features stand out. Without this, all algorithms, particularly generating lines on the edges, yielded excessive noise and less desirable results. For this, a median blur and bilateral filter was used. These were performed sequentially on the image twice. The positive effect of reducing feature noise can be seen in the figure below. After blurring, features like eyes do not show up well, leading to more clear results. Other features like the color difference we care about are still clear, however.



After filtering the image, a color threshold is applied to the image to produce a mask. This color threshold separates the bridge--namely the res strips on the sides--from the rest of the image. This mask is then placed on top of a white background. This is because most of the resulting noise features are white or close to white. Doing this step reduces the effect of this noise. This noise reduction is significant for both the gap finding method we used to attempt to stay on the bridge and the line deriving from edges we implemented to find was not a good fit for the problem. The output of this mask can be seen below. The main problem with robustness arose here. In certain lighting conditions, it was extremely hard to tell the bridge from the room. Further, the upper and lower bound needed to find the bridge changed with lighting conditions. However, since the strips that noted the bridge sides were denoted with red color, it was a necessary step.
 

After this, there was still significant noise in the upper portion of the image. This was addressed in the simplest way possible: cropping the image! This method introduces a potentially large failure mode where if the bridge is outside the cropping region, the robot will not be able to stay on the bridge. It is the most effective method of removing the background noise, though. The cropped image can be seen in the image below. The noise from the ceiling in the previous image is removed. 



Finally, to determine where the bridge was, the sum of all pixel values in each column was computed. This was done with  numpy sum operation to improve efficiency. The average pixel value was then found by dividing this sum by the number of elements in the column. This was done to normalize the image and gave the value a meaning if the height of the cropped image changed. Additionally, it made this value more human readable. Next, a threshold was defined. If the computed average for a column was below this, it was considered to be a bridge edge. If it was above this, it as a gap. At all transitions from in a gap to out of a gap, the length of the gap was checked. If it was longer than the previously longest gap, the value for the longest gap was updated and the new start and end points were also recorded. Once all columns in the image were iterated through, the computed values were drawn on the image to assist with human validation of the method. The green box in the figure below is the largest gap calculated with this method. The red, vertical line is the center of the image. This is relevant, albeit uninteresting, because it shows what the neato is heading toward and directly in front of. The blue line is the calculated center of the largest gap. This is the point which the neato is aiming for so it will not fall off the bridge. All this can be seen in the image below. One important note, this is only shown in the top 30 pixels so the rest of the image is easily visible. However, the vertical lines and rectangle apply to vertical, one pixel wide slices through the image. 


This computed data was translated to motion on the part of the robot through a simple proportional controller. The difference between the center of the largest gap and center of the gap is computed. This is multiplied. If the robot is too far to one side, it turns to correct its trajectory. This comparison method was used because without using a prior trajectory, it would be weird to set a steering angle from pixel values. Therefore, the proportional constant makes difference from pixel values be a reasonably amount to turn. 

In developing this algorithm, other approaches were also investigates. One of these that was fully implemented was detecting edges in the image and fitting lines to these. This was somewhat effective except that getting continuous lines was very challenging with the image of the bridge from the neeto camera. This resulted in clusters of very short lines. Additionally, features in the ceiling like lights had a tendency to be detected as well and had an increased adverse impact on performance over the column summing, gap method employed. The image was prepared for detecting edges in the same way as presented above for finding the largest gap. The resulting line segments were too short to be useful. Some potential methods to use these line segments were developed, but the gap method seemed to hold much more progress, so the deriving line method was abandoned. 


##Challenges
It took a lot longer than we had expected to finish getting ar_pose installed and working.  We spent a significant amount of time learning how to implement the ROS packages we used, and we ran into some frustrating errors with them that were very difficult to track down.  Specifically, the move_base package often had errors that caused the robot to default to “recovery mode”, or spinning in circles until it decides it is able to travel to the goal.  This made the obstacle a lot slower than we expected.

Additionally, we struggled with some of the logic of the individual obstacles, each of which presented some unique challenges.  The tunnel dealt with some unpredictable blips in Lidar data.  In the dominoes, even after we were able to get the Neato to travel to the obstacle, we had some difficulties with setting up the Finite State Machine correctly such that it knew how to move after it had knocked over a single domino.  On the bridge, we had some trouble getting the image filtering correct such that it knew which direction to travel in.

The image processing for the bridge was extremely challenging. There were the simple  challenges like a mistake with mutable objects making development take longer and concepts which are conceptually challenging like bitwise operations of deriving relative location from poorly defined points in a 2D plane. Further, tuning the many parameters for the openCV methods when the lighting so frequently changed was next to impossible and seriously impeded testing functionality. 

Overall, we never got all three systems working together.  However, we did get the robot to: start in a random pose, find the the tunnel, align itself with the tunnel, go through the tunnel successfully, recognize that it had successfully completed the tunnel, look around for the next obstacle, find the dominoes, and align itself with the dominoes.  We then had other tests where it started in a random pose, aligned itself with the dominoes, processed the dominoes, drove up to them as intended, and when finished returned to the starting pose and began looking for the next obstacle.  What we believe from these two examples is that our code is able to do the functions we set out for it, although not 100% reliably.  However, we think that some of the challenge in getting the robot perfectly aligned for the second obstacle after completing the first is partially due to using dead reckoning, and the robot not being perfectly on course where it thinks it is.  It could also be due to some physical problems or connectivity problems - our move_base package often has difficulty in some cases, and it just spins the robot until it figures out what to do next.

##Future Work
First of all, we would have liked to complete our goal of getting all three obstacles integrated.  We got two integrated, to some extent, but they are not quite as robust as we had expected or hoped.  We came very close to having all three working together in some form, but that would be the first reasonable thing to work on in the future.
Additionally, had we had more time, we would have definitely liked to work on adding more obstacles, as well as making what we currently have more robust.  Our robot successfully completes the obstacles sometimes, but it’s not 100% accurate, and that is a flaw. It would also be interesting to add memory to the system, so that it wouldn’t complete the same obstacle twice in a row, and a more interesting finding algorithm, so that it doesn’t just spin in circles until it sees a fiducial.
If we had a lot of time, it would be very cool to eliminate the fiducials on the dominos and rely solely on object recognition and color detection. It would also be fantastic to make the system more robust to different lighting conditions. Not working because someone changed the location of the obstacle in the room is not really permissible if this were available to consumers. To address this, it would potentially be nice to see the effect of implementing automatic tuning of parameters. A simple GUI was developed that allowed us to tweak the color threshold, but there were other parameters that also affected the performance of the bridge traversing such that it never operated remotely with all parameters close to correct. The best we were able to achieve was “sort of good enough.” This is unfortunate and limited the function of the robot. 

##Lessons Learned
We learned a lot about finite state controllers, especially in the Domino code, because it juggles a lot of states, and switching them in both callback functions as well as normal functions.  Learning about collecting video data from the robot was a new experience for some of us, as well as learning how to read and use fiducials to control the robot and give it information. We also gain a lot of experience in integrating 3rd party ros packages. We also learned a lot of new computer vision functionality and innovative ways to apply these methods. For example, trying to center a robot between sporadic line segments or even generating line segments from an image was not something some of us had ever done before. 
